\documentclass[oneside]{book}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{color}
\usepackage{multicol}
\usepackage{ragged2e}
\usepackage{listings}
\usepackage{pdfpages}
\title{Notes de Présentation INF 8225}
\date{2018-01-01}
\author{Olivier Sirois}
\setlength\parindent{0pt}
\makeindex
\pagenumbering{arabic}
\begin{document}
\setcounter{page}{1}
\maketitle
\tableofcontents


\section{VGG net}
Réseau de neurones convolutif profond utilisant des petits filtres. On adapte la profondeur à la richesse des données pour éviter la saturation \textit{généralement très profond}.\\

S'applique à classification + localisation. 

\subsection{Arch}
\paragraph{Input} images 224x224 RGB preprocessed
\paragraph{Channel} propres à chaque couche (64, 128, 256...)
\paragraph{Stride} 1
\paragraph{Zero padding} Pour préserver la dimension de l'image

\paragraph{Fonction d'activation} ReLU
\paragraph{Max Pooling} appliqué sur 2x2
\paragraph{FC et softmax} pour loss

Différentes configuration sur utiliser pour adapter les données. 16 couche de convolution.. \\

l'objectif est de faire des petits filtre et de les mettres en profondeurs.\\

\subsection{Conclusion}
Prondeur permet d'améliorer les performance d'un réseau de neurones convolutif au prix de \textbf{petits filtres}. Plus performant que GoogleNet lors de son arrivé.

\section{Dropout}
Références, Goodfellow et al. chapitre sur la régularisation...\\

\subsection{Motivation}
Même motivation que les autres méthodes de régularisation. Réduit l'overfitting et augmente la généralistion sur le model.\\

\subsection{Principe}
Durant l'entraînement, on applique qu'une sous-partie du réseau déterminé aléatoirement. On 'drop' certain neurones dans le réseau lors de l'entraînement. l'objectif est que le réseau est capable d'apprendre avec l'ensemble de ses neurones au lieu d'avoir certaines neurones qui fait certaines tâches. C'est équivalent à entraîner plusieurs réseaux qui partagent des nodes, puis d'utiliser une moyenne géométrique sur l'assemblage total (bootstrap aggregating??).\\

\subsection{Implémentation}
On multiplie la sortie de notre neurones avec un masque (1 ou 0) avec une propabilité $p$.\\


\subsection{Résultats} 
On peut voir une amélioration entre 20 et 60\%. Par contre, sa peut prendre beaucoup plus de neurones pour avoir la même performance... \\

\subsection{Autres techniques} 
Augmentation de données, pré-entraînement non-supervisé.. Aucune des techniques ne sont mutuellement exclusive.\\

\section{Inception}
Architecture de google.. 
\subsection{Motivation}
Pour améliorer googlenet, essaie de réduire les dimensions des matrice d'entré. Essaie de factoriser les filtre de convolution pour améliorer les performances ?? (whatever that means)..\\

En factorisation, on obtient de convolution plus petites. On effectue une factoriation spatial dans un filtre asymmétrique.\\

(remplace des convolution de 5x5 par 2 convolution de 3x3. cette opération amène un gains relatif d'environ 28\%. 
\subsection{Arch}
Plusieurs convolution de taille différente.. (factorisation)\\

Utilisation de classificateur auxiliaire (en parallèle??)..  \\

Pas compris.. (les gens qui ont présenté n'ont pas trop compris non plus)\\
\subsection{Resultats}
Plus grande performance dans les résaux ayant un plus grand champ réceptif.

\subsection{Conclusion}
facorisation des opérations de convolution en convolution plus petite, ce qui permet de rendre model beaucoup plus compact. Ce qui en résulte à une plus grande facilité à mettre à l'échelle des réseaux de grandes tailles.\\

\section{Batch Norm}
\subsection{Motivation}
La mise à l'échelle de différents features en entré d'un réseau permet l'apprentissage. On eveut l'appliquer au Deep Learning\\

l'idée est de normaliser les input avant l'activation pour qu'ils ait une distribution précise (moyenne = 0, variance = 1). 

\subsection{résultats}
Avec batch norm, on converge rapidement à un model qui apprend. BN rend la distribution de la sortie beaucoup plus stable. \\

l'application de BN sur inception permet d'améliorer la puissance de l'apprentissage. voir le modèle dans la présentation pour voir la vitesse de convergence. La performance augmente plus rapidement et se stabilise beaucoup mieux (plus performant). Sa converge beaucoup plus vite car la stabilité de la distribution des sorties est beaucoup plus stable alors on peut se permettre d'augmenter le taux d'apprentissage.\\

L'utilisation de la normalisation par lot peut nous permettre de nous passer de Dropout pour régulariser. Elle facilite aussi l'utilisation des fonctions sigmoide dans les réseaux profond en plus de pouvoir être appliqué aux couches de convolutions.

\subsection{Conclusion}

\section{ResNet}

\subsection{Motivation}
l'utilisation de plusieurs couches en profondeurs permet d'augmenter la capacité en utilisant moins de liens, par contre la profondeur fait en sorte qu'on a souvent un problème de 'vanishing gradient' et un apprentissage très lent.\\

Pour remédier au problème, On ajoute des connections entre des layers de différentes profondeur pour que le gradient puisse se propager dans les couches plus hautes sans avoir la dégradation causées par toutes les couches lui suivants.\\

$H(x) = F(x) + \underline{\textbf{x}}$\\

\subsection{arch}
suit les mêmes guideline que le VGG. On applique aussi la normalisation par lot pour normalisé les sorties obtenus. 

\subsection{résultats et conclusion}

ResNet à dépassé de loin toutes les architectures de reconnaissance à l'époque. PAr contre, elle contenait 157 couches de profondeur comparés aux 15-20 des autres architectures. Cette architecture permet de contourné le problème de disparition de gradient (vanishing gradient).\\

ResNet s'inspire beaucoup du \textit{highway Network}. ResNet est plus performance que Highway sur CIFAR-10. (6.6\% vs. 8.7\%). \\

Certains rafinement en enlevant des fonctions d'activation permet d'augmenter la performance de manière significative.\\

ResNExt : utilisation des couches en parallèle en plus d'avoir des connections résiduels. 
\section{Fully convolutional network for semantic segmentation}

\subsection{Motivation et Intuition}
Des couches fully connected utilise un noyau de la taille de l'image. En ayant un filtre pour chacune des classes à prédire, on peut avoir une carte avec plus de localités. la classification se fait de manière plus locales. \\

Une couche fully connected = ??\\

Trop rapide.. pas toute compris..

\section{Convolutional Netowkrs for Biomedical image segmentation}

\subsection{Motivation}
Utilisation des réseuax de neurones convolutif pour segmenter les iamges médicales. Pour faire les reconnaissance de features pouvant être des précurseur à certaines pathologies.\\

\subsection{architecture}
Voir photos. on copie l'image d'entrée à la sortie. Sauf qu'on a des branches résiduel qui permet la classification de features contextuel sans affecter l'image originale. 

\section{YOLO - you only look once, unified real time object detection}

\subsection{Motivation + intuition}
Provient d'un besoin en temps réel de modèle capable de faire:
\begin{itemize}
\item Conduite autonome
\item Dispositif d'assistance
\item ..
\end{itemize}

Comparés au approches classique, YOLO fait un seul problème de régression une seule fois dans l'image. Il fait des prédictions directement à partir d'une seule image. 45 FPS pour YOLO

\subsection{Arch}
Utilise seulement un seul réseau convolutif ayant comme input un image complet. Il prédit des boîte délimitant la classification ainsi que sa probabilité. \\
Étape:\\
 \begin{itemize}
 \item Divison de l'image en grille sxs
 \item Chaque cell prédit un certain nombre de boite de délimitation
 \item Évaluation d'un taux de confiance
 \item Évaluation de la probabilité de la classe de l'objet
 \end{itemize}
 
Fonctions d'activation : ReLU or leaky ReLU\\

\subsection{Résultats}
Avantages:
\begin{itemize}
\item Rapide
\item Simple à construire
\item Généralisable sur différents domaines
\item Assez bonne précision de détection d'object
\item Fausse détection relativement faible
\end{itemize}

Désavantage:
\begin{itemize}
\item Loss function peu puissante
\item Caractéristique relativement grossière
\item Forte contraintes spatiale
\item Principale source d'erreur: les localisations


\section{Machine Theory of Mind}
Concept qui est lier à la psychologie. Notre compréhension des autres nous permet d'interagir avec les autres. Le papier propose une manière d'interfacer les ordinateurs avec les humains.

\subsection{Objectif}
Produire un observateur qui peut aprendre autonement. On essaie d'apprendre à apprendre à modéliser des agents intelligents.\\


\chapter{Semaine 2}
\end{itemize}
\end{document}
